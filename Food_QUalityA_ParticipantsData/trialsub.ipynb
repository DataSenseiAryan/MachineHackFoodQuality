{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kFEo8EwDLq1H",
    "outputId": "3a82fbf3-bed9-41fb-f462-205cb544e604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import category_encoders\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import matplotlib as plt\n",
    "from random import randint\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoz1hAGBLq1Q"
   },
   "outputs": [],
   "source": [
    "# Read the file into a variable fifa_data\n",
    "filepath = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/Data_Train.xlsx\"\n",
    "data = pd.read_excel(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file into a variable fifa_data\n",
    " \n",
    "test = pd.read_excel(\"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/Data_Test.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hbEjJjM0Lq1X"
   },
   "outputs": [],
   "source": [
    "submission= pd.read_excel('/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/Sample_Submission.xlsx')\n",
    "# data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQx71DcSLq1v"
   },
   "outputs": [],
   "source": [
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# clf1 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "#                        max_depth=None, max_features=11, max_leaf_nodes=None,\n",
    "#                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                        min_samples_leaf=8, min_samples_split=2,\n",
    "#                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "#                        random_state=None, splitter='best')\n",
    "\n",
    "# clf2 = BaggingClassifier(base_estimator=clf1, n_estimators=100, max_samples=1.0, max_features=1.0, \n",
    "#                          bootstrap=True,\n",
    "#                          bootstrap_features=False, oob_score=True, warm_start=False,\n",
    "#                          n_jobs=-1, random_state=None, verbose=0)\n",
    "\n",
    "# clf3=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.8, gamma=0.5,\n",
    "#               learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
    "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
    "#               nthread=None, objective='multi:softprob', random_state=0,\n",
    "#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#               silent=None, subsample=0.6, verbosity=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaOKY4bpLq11"
   },
   "outputs": [],
   "source": [
    "def dropcol(train,test):\n",
    "    train['logID'] = np.log1p(train['ID'])\n",
    "    test['logID'] = np.log1p(test['ID'])\n",
    "    cols_to_drop = ['Date','ID']\n",
    "    dtrain = train.drop(cols_to_drop,axis =1)\n",
    "    dtest = test.drop(cols_to_drop,axis =1)\n",
    "    \n",
    "    print(dtrain.shape,dtest.shape)\n",
    "\n",
    "    return dtrain,dtest\n",
    "\n",
    "def catVar1(data):\n",
    "    categorical_colsT1 = [cname for cname in data.columns if\n",
    "                    data[cname].nunique() <=10 and \n",
    "                    data[cname].dtype == \"object\"]\n",
    "    return categorical_colsT1\n",
    "\n",
    "def catVar2(data):\n",
    "    categorical_colsT2 = [cname for cname in data.columns if\n",
    "                    data[cname].nunique() >10 and \n",
    "                    data[cname].dtype == \"object\"]\n",
    "    return categorical_colsT2\n",
    "\n",
    "def NumVar(data) :\n",
    "    numerical_cols = [cname for cname in data.columns if \n",
    "    data[cname].dtype in ['int64', 'float64']]\n",
    "    return numerical_cols\n",
    "\n",
    "def imputer(train,test):\n",
    "    \n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    numerical_cols = NumVar(train)\n",
    "    my_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    imputed_train = pd.DataFrame(my_imputer.fit_transform(train))\n",
    "    imputed_test = pd.DataFrame(my_imputer.transform(test))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_train.columns = train.columns\n",
    "    imputed_test.columns = test.columns\n",
    "    \n",
    "    #restoring datatypes \n",
    "    imputed_train[numerical_cols] = imputed_train[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    imputed_test[numerical_cols] = imputed_test[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    print(imputed_train.shape,imputed_test.shape)\n",
    "    return imputed_train,imputed_test \n",
    "\n",
    "def robustlabelencoder(train,test):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    label_enc = LabelEncoderExt()\n",
    "    cols = catVar2(train)\n",
    "    print(cols)\n",
    "    for col in cols:\n",
    "        label_enc.fit(train[col])\n",
    "        train[col] = label_enc.transform(train[col])\n",
    "        test[col] = label_enc.transform(test[col])\n",
    "        \n",
    "    print(train.shape,test.shape)\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "def normalabelencoder(train,test,cols):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    label_enc = LabelEncoder()\n",
    "    \n",
    "    print(cols)\n",
    "    for col in cols:\n",
    "        label_enc.fit(train[col])\n",
    "        train[col] = label_enc.transform(train[col])\n",
    "        test[col] = label_enc.transform(test[col])\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "\n",
    "\n",
    "def ohebygetdummis(train,test):\n",
    "    pass\n",
    "\n",
    "def onhotencoder(train,test):\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    object_cols = catVar1(train) #catVar1 gives desired categorical column and not all object columns\n",
    "    print(object_cols)\n",
    "    \n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[object_cols]))\n",
    "    OH_cols_test = pd.DataFrame(OH_encoder.transform(test[object_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = train.index\n",
    "    OH_cols_test.index = test.index\n",
    "\n",
    "    ##hack for restoring columns names just like get dummies\n",
    "    column_name = OH_encoder.get_feature_names(object_cols)\n",
    "    OH_cols_train.columns = column_name\n",
    "    OH_cols_test.columns = column_name\n",
    "    \n",
    "\n",
    "    # Remove desired categorical columns (will replace with one-hot encoding)\n",
    "    num_train = train.drop(object_cols, axis=1)\n",
    "    num_test = test.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical/remaining features\n",
    "    OH_train = pd.concat([num_train, OH_cols_train], axis=1)\n",
    "    OH_test = pd.concat([num_test, OH_cols_test], axis=1)\n",
    "    \n",
    "    print(OH_train.shape,OH_test.shape)\n",
    "    \n",
    "    \n",
    "    return OH_train,OH_test\n",
    "\n",
    "def missingcheck(data):\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    percent_1 = data.isnull().sum()/data.isnull().count()*100\n",
    "    percent_2 = (np.round(percent_1, 1)).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%']) #ptr\n",
    "    return missing_data\n",
    "\n",
    "def targetencoding(train,test,y_train):\n",
    "    import category_encoders as ce\n",
    "    # Create the encoder itself\n",
    "    cat_features = catVar2(train)\n",
    "    print(f'targest emcoding for features {cat_features}')\n",
    "    target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "    \n",
    "\n",
    "    # Fit the encoder using the categorical features and target\n",
    "    target_enc.fit(train[cat_features], y_train)\n",
    "    \n",
    "\n",
    "    # Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "    traintrgtenc = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "    testtrgtenc = test.join(target_enc.transform(test[cat_features]).add_suffix('_target'))\n",
    "\n",
    "    traintrgtenc = traintrgtenc.drop(cat_features, axis =1)\n",
    "    testtrgtenc = testtrgtenc.drop(cat_features, axis =1)\n",
    "\n",
    "    \n",
    "    print(traintrgtenc.shape,testtrgtenc.shape)\n",
    "    \n",
    "    return traintrgtenc,testtrgtenc\n",
    "\n",
    "\n",
    "def special(train,test):\n",
    "    train['Geo_Loc'] = train['Geo_Loc'].str.replace(r'\\D', '')\n",
    "    train['Geo_Loc'] = pd.to_numeric(train['Geo_Loc'], errors='coerce') \n",
    "    \n",
    "    test['Geo_Loc'] = test['Geo_Loc'].str.replace(r'\\D', '')\n",
    "    test['Geo_Loc'] = pd.to_numeric(test['Geo_Loc'], errors='coerce')\n",
    "    \n",
    "    train['Date'] = pd.to_datetime(train['Date'] ,errors='coerce')\n",
    "    test['Date'] = pd.to_datetime(test['Date'] ,errors='coerce')\n",
    "    \n",
    "    train = train.assign(\n",
    "               hour=train.Date.dt.hour,\n",
    "               day=train.Date.dt.day,\n",
    "               month=train.Date.dt.month,\n",
    "               year=train.Date.dt.year\n",
    "                        )\n",
    "    \n",
    "    test = test.assign(\n",
    "               hour=test.Date.dt.hour,\n",
    "               day=test.Date.dt.day,\n",
    "               month=test.Date.dt.month,\n",
    "               year=test.Date.dt.year\n",
    "                        )\n",
    "    print(train.shape,test.shape)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0gqe6UoLq15"
   },
   "outputs": [],
   "source": [
    "# def syntheticvariable(train , y ):\n",
    "#     from imblearn.over_sampling import SMOTENC\n",
    "#     from collections import Counter\n",
    "#     categorical_features = [cname.index for cname in train.columns if train[cname].dtype == \"object\"]\n",
    "#     out = np.argwhere(train.columns.isin(categorical_features)).ravel().tolist()\n",
    "    \n",
    "#     smote_nc = SMOTENC([3, 5, 6, 8, 10], random_state=0)  #instead of passing out i need to hardcode it.\n",
    "\n",
    "#     X_resampled, y_resampled = smote_nc.fit_resample(train,y)\n",
    "    \n",
    "#     print(sorted(Counter(y_resampled).items()))\n",
    "    \n",
    "#     return X_resampled,y_resampled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glt8kTxZLq1-"
   },
   "outputs": [],
   "source": [
    "#####crete new features###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTTM9ah-Lq2F"
   },
   "outputs": [],
   "source": [
    "def createIntercations(train,test,cat_features):\n",
    "    import itertools\n",
    "    print(f\"Creating features on {cat_features}, with combination 2 for training data /n\")\n",
    "    interactionstrain = pd.DataFrame(index=train.index)\n",
    "    \n",
    "    for col1 ,col2 in  itertools.combinations(cat_features,2):   \n",
    "        newcolname = col1 + \"_\" + col2 \n",
    "        new_values = train[col1].map(str) + \"_\" + train[col2].map(str)\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "    \n",
    "    train_df = train.join(interactionstrain)\n",
    "    \n",
    "    print(f\"Creating features on {cat_features}, with combination 2 for testing data\")\n",
    "    interactionstest = pd.DataFrame(index=train.index)\n",
    "    \n",
    "    for col1 ,col2 in  itertools.combinations(cat_features,2):   \n",
    "        newcolname = col1 + \"_\" + col2 \n",
    "        new_values = test[col1].map(str) + \"_\" + test[col2].map(str)\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "    test_df = test.join(interactionstest)\n",
    "    \n",
    "    print(train_df.shape,test_df.shape)\n",
    "    \n",
    "    \n",
    "    return train_df,test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6CI5m2cLq2K"
   },
   "outputs": [],
   "source": [
    "def createmeanfestures(train,test ,features):\n",
    "  \n",
    "  cols = features\n",
    "  interactionstrain = pd.DataFrame(index=train.index)\n",
    "  interactionstest = pd.DataFrame(index=test.index)\n",
    "\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_mean\"   \n",
    "        new_values = train[col].mean()\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_mean\"   \n",
    "        new_values = test[col].mean()\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "\n",
    "  train_df = train.join(interactionstrain)\n",
    "  test_df = test.join(interactionstest)\n",
    "\n",
    "  print(train_df.shape,test_df.shape)\n",
    "\n",
    "  return train_df,test_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createlogfestures(train,test,features):\n",
    "  \n",
    "  cols = features\n",
    "  interactionstrain = pd.DataFrame(index=train.index)\n",
    "  interactionstest = pd.DataFrame(index=test.index)\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_log\"   \n",
    "        new_values = np.log1p(train[col])\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_log\"   \n",
    "        new_values = np.log1p(test[col])\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "\n",
    "  train_df = train.join(interactionstrain)\n",
    "  test_df = test.join(interactionstest)\n",
    "\n",
    "  print(train_df.shape,test_df.shape)\n",
    "\n",
    "\n",
    "  return train_df ,test_df\n",
    "\n",
    "def createsqrtfeatures(train,test,features):\n",
    "    \n",
    "\n",
    "  cols = features\n",
    "  interactionstrain = pd.DataFrame(index=train.index)\n",
    "  interactionstest = pd.DataFrame(index=test.index)\n",
    "  \n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_sqrt\"   \n",
    "        new_values = np.sqrt(train[col])\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_sqrt\"   \n",
    "        new_values = np.sqrt(test[col])\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "\n",
    "  train_df = train.join(interactionstrain)\n",
    "  test_df = test.join(interactionstest)\n",
    "\n",
    "  print(train_df.shape,test_df.shape)\n",
    "\n",
    "\n",
    "  return train_df ,test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4qIejjhLq2O"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "    \n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    #model = RandomForestClassifier(random_state=0)\n",
    "    #model.fit(X_train, y_train)\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "    clf1 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=None, max_features=25, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=8, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "    clf2 = BaggingClassifier(base_estimator=clf1, n_estimators=10, \n",
    "                             bootstrap=True,\n",
    "                             bootstrap_features=False, oob_score=True, warm_start=False,\n",
    "                             n_jobs=-1, random_state=786, verbose=0)\n",
    "    clf2.fit(X_train,y_train)\n",
    "\n",
    "    preds = clf2.predict(X_valid)\n",
    "    target_names = ['class 0', 'class 1', 'class 2','class 3', 'class 4', 'class 5', 'class 6']\n",
    "    print(classification_report(y_valid, preds, target_names=target_names,labels= [0,1,2,3,4,5,6]))\n",
    "    return clf2.score(X_valid,y_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jG0IPOVrLq2T"
   },
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid\n",
    "#clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Inspection_Results\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "train = data.drop(['Inspection_Results'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "3aCJ8JExLq2X",
    "outputId": "01e10416-b376-4745-ff76-9a5d40cf0005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 18) (49148, 18)\n"
     ]
    }
   ],
   "source": [
    "trainS, testS = special(train ,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Os_fd0AYLq2e",
    "outputId": "76b63eee-8257-4db7-96ca-2482c23dfb72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 17) (49148, 17)\n"
     ]
    }
   ],
   "source": [
    "trainSd, testSd = dropcol(trainS,testS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ro7b9oEqLq2k",
    "outputId": "d2dd3845-23e1-4a62-c9af-ca38ac559e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 17) (49148, 17)\n"
     ]
    }
   ],
   "source": [
    "trainSdIm, testSdIm = imputer(trainSd, testSd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "dxkAfjvCLq2q",
    "outputId": "0f2cd5fe-7e65-4330-de46-678b6cc69c44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                   Total    %\n",
       " logID                  0  0.0\n",
       " LocationID             0  0.0\n",
       " FacilityID             0  0.0\n",
       " FacilityName           0  0.0\n",
       " Type                   0  0.0\n",
       " Street                 0  0.0\n",
       " City                   0  0.0\n",
       " State                  0  0.0\n",
       " Reason                 0  0.0\n",
       " year                   0  0.0\n",
       " SectionViolations      0  0.0\n",
       " RiskLevel              0  0.0\n",
       " Geo_Loc                0  0.0\n",
       " hour                   0  0.0\n",
       " day                    0  0.0\n",
       " month                  0  0.0\n",
       " LicenseNo              0  0.0,                    Total    %\n",
       " logID                  0  0.0\n",
       " LocationID             0  0.0\n",
       " FacilityID             0  0.0\n",
       " FacilityName           0  0.0\n",
       " Type                   0  0.0\n",
       " Street                 0  0.0\n",
       " City                   0  0.0\n",
       " State                  0  0.0\n",
       " Reason                 0  0.0\n",
       " year                   0  0.0\n",
       " SectionViolations      0  0.0\n",
       " RiskLevel              0  0.0\n",
       " Geo_Loc                0  0.0\n",
       " hour                   0  0.0\n",
       " day                    0  0.0\n",
       " month                  0  0.0\n",
       " LicenseNo              0  0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingcheck(trainSdIm),missingcheck(testSdIm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2aw1FGA0Lq2u",
    "outputId": "c38efd08-d954-41a7-a794-b64e8b14eb12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features on ['SectionViolations', 'RiskLevel', 'Reason'], with combination 2 for training data /n\n",
      "Creating features on ['SectionViolations', 'RiskLevel', 'Reason'], with combination 2 for testing data\n",
      "(147443, 20) (49148, 20)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn1,testSdImIn1 = createIntercations(trainSdIm, testSdIm,cat_features = ['SectionViolations','RiskLevel','Reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3kYy-ZzwLq2y",
    "outputId": "372d0c76-8130-4dca-be62-f6c58ee192de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features on ['State', 'City', 'Street', 'LocationID', 'Geo_Loc'], with combination 2 for training data /n\n",
      "Creating features on ['State', 'City', 'Street', 'LocationID', 'Geo_Loc'], with combination 2 for testing data\n",
      "(147443, 30) (49148, 30)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12,testSdImIn12 = createIntercations(trainSdImIn1,testSdImIn1,cat_features = ['State','City','Street','LocationID','Geo_Loc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaK3rthjLq22"
   },
   "outputs": [],
   "source": [
    "# X_trainSDrIMintertgenc,X_validSDrIMinter2tgenc = targetencoding(X_trainSDrIMinter2,X_validSDrIMinter2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "yWqJ47VYLq29",
    "outputId": "55b003e3-1719-4c87-9bf8-431d9b54cd12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Reason', 'SectionViolations_RiskLevel', 'SectionViolations_Reason', 'RiskLevel_Reason', 'State_Street', 'State_LocationID', 'State_Geo_Loc', 'City_Street', 'City_LocationID', 'City_Geo_Loc', 'Street_LocationID', 'Street_Geo_Loc', 'LocationID_Geo_Loc']\n",
      "(147443, 30) (49148, 30)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12lb,testSdImIn12lb = robustlabelencoder(trainSdImIn12,testSdImIn12 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zqc8oJbZLq3C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-VY5Kj5Lq3G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "u3mh8M9DLq3J",
    "outputId": "7ec7345a-4dec-4561-c745-5e05383d944c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['City', 'State', 'RiskLevel', 'State_City']\n",
      "(147443, 38) (49148, 38)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12lbohe,testSdImIn12lbohe = onhotencoder(trainSdImIn12lb,testSdImIn12lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "8SeFb6P7wSDj",
    "outputId": "5246a922-5689-41e3-f144-15112a193360"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LicenseNo', 'FacilityID', 'FacilityName', 'Type', 'Street',\n",
       "       'LocationID', 'Reason', 'SectionViolations', 'Geo_Loc', 'hour', 'day',\n",
       "       'month', 'year', 'logID', 'SectionViolations_RiskLevel',\n",
       "       'SectionViolations_Reason', 'RiskLevel_Reason', 'State_Street',\n",
       "       'State_LocationID', 'State_Geo_Loc', 'City_Street', 'City_LocationID',\n",
       "       'City_Geo_Loc', 'Street_LocationID', 'Street_Geo_Loc',\n",
       "       'LocationID_Geo_Loc', 'City_id-11235901', 'City_id-11275913',\n",
       "       'State_id_1890134', 'State_id_1890135', 'RiskLevel_High',\n",
       "       'RiskLevel_Low', 'RiskLevel_Medium', 'RiskLevel_Uncertain',\n",
       "       'State_City_id_1890134_id-11235901',\n",
       "       'State_City_id_1890134_id-11275913',\n",
       "       'State_City_id_1890135_id-11235901',\n",
       "       'State_City_id_1890135_id-11275913'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = trainSdImIn12lbohe.columns\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7VcTkYBfuoxw",
    "outputId": "ab52ef67-905f-4a21-a033-af890abee230"
   },
   "outputs": [],
   "source": [
    "# featres = ['LicenseNo',\n",
    "#            'FacilityID', \n",
    "#            'FacilityName', \n",
    "#            'Type', \n",
    "#            'Street',\n",
    "#            'LocationID', \n",
    "#            'Reason', \n",
    "#            'SectionViolations', \n",
    "#            'Geo_Loc',\n",
    "#            'hour', \n",
    "#            'day',\n",
    "#            'month', \n",
    "#            'year', \n",
    "            \n",
    "#            'SectionViolations_RiskLevel',\n",
    "#            'SectionViolations_Reason',\n",
    "#            'RiskLevel_Reason', \n",
    "#            'State_Street',\n",
    "#            'State_LocationID', \n",
    "#            'State_Geo_Loc',\n",
    "#            'City_Street', \n",
    "#            'City_LocationID',\n",
    "#            'City_Geo_Loc', \n",
    "#            'Street_LocationID', \n",
    "#            'Street_Geo_Loc',\n",
    "#            'LocationID_Geo_Loc', \n",
    "#            'City_id-11235901',\n",
    "#            'City_id-11275913',\n",
    "#            'State_id_1890134', \n",
    "#            'State_id_1890135', \n",
    "#            'RiskLevel_High',\n",
    "#            'RiskLevel_Low',\n",
    "#            'RiskLevel_Medium',\n",
    "#            'RiskLevel_Uncertain',\n",
    "#                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'State_City_id_1890134_id-11235901',\n",
    "#            'State_City_id_1890134_id-11275913',\n",
    "#            'State_City_id_1890135_id-11235901',\n",
    "#            'State_City_id_1890135_id-11275913'\n",
    "# drop these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropcol2(train,test):\n",
    "    cols_to_drop =['hour',\n",
    "                   'State_City_id_1890134_id-11235901',\n",
    "                   'State_City_id_1890134_id-11275913',\n",
    "                   'State_City_id_1890135_id-11235901',\n",
    "                   'State_City_id_1890135_id-11275913'\n",
    "]\n",
    "    dtrain = train.drop(cols_to_drop,axis =1)\n",
    "    dtest = test.drop(cols_to_drop,axis =1)\n",
    "    \n",
    "    print(dtrain.shape,dtest.shape)\n",
    "\n",
    "    return dtrain,dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 33) (49148, 33)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12lboheD,testSdImIn12lboheD = dropcol2(trainSdImIn12lbohe,testSdImIn12lbohe )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZjbAtYuUuo_P",
    "outputId": "0dc1abab-9ba4-4962-ab73-7b874577c1a1"
   },
   "outputs": [],
   "source": [
    "#trainSdImIn12lboheLog,testSdImIn12lboheLog = createlogfestures(trainSdImIn12lbohe,testSdImIn12lbohe,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PnoVbxPJ0S0H",
    "outputId": "ed8f2533-2ddc-4587-f9b1-dd7a3864d5a4"
   },
   "outputs": [],
   "source": [
    "# trainSdImIn12lbohe,testSdImIn12lbohe = createsqrtfeatures(trainSdImIn12lbohe,testSdImIn12lbohe,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGt_o723Lq4e"
   },
   "outputs": [],
   "source": [
    "def featureselection(train,y_train):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.feature_selection import chi2\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "    feature_name = train.columns.tolist()\n",
    "    def cor_selector(X, y):\n",
    "        cor_list = []\n",
    "        feature_name = X.columns.tolist()\n",
    "\n",
    "        # calculate the correlation with y for each feature\n",
    "        for i in X.columns.tolist():\n",
    "            cor = np.corrcoef(X[i], y)[0, 1]\n",
    "            cor_list.append(cor)\n",
    "        # replace NaN with 0\n",
    "        cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "        # feature name\n",
    "        cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-100:]].columns.tolist()\n",
    "        # feature selection? 0 for not select, 1 for select\n",
    "        cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "        return cor_support, cor_feature\n",
    "\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=7 )\n",
    "    model_rf = SelectFromModel(rf ,threshold ='1.25*median')\n",
    "    X_new_rf = model_rf.fit(train,y_train)\n",
    "    embeded_rf_support = model_rf.get_support()\n",
    "    embeded_rf_feature = train.loc[:,embeded_rf_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    model_lgbc = SelectFromModel(lgbc,threshold = '1.25*median')\n",
    "    X_new_lgbc = model_lgbc.fit(train,y_train)\n",
    "    lgbc_rf_support = model_lgbc.get_support()\n",
    "    lgbc_rf_feature = train.loc[:,lgbc_rf_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    cor_support, cor_feature = cor_selector(train, y_train)\n",
    "    \n",
    "    \n",
    "    X_norm = MinMaxScaler().fit_transform(train)\n",
    "    \n",
    "    chi_selector = SelectKBest(chi2, k=80)\n",
    "    chi_selector.fit(X_norm, y_train)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = train.loc[:,chi_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=100, step=10, verbose=5)\n",
    "    rfe_selector.fit(X_norm, y_train)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = train.loc[:,rfe_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\",solver = 'liblinear'), '1.25*median')\n",
    "    embeded_lr_selector.fit(X_norm, y_train)\n",
    "\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    embeded_lr_feature = train.loc[:,embeded_lr_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n",
    "                                        'Random Forest':embeded_rf_support, 'LightGBM':lgbc_rf_support})\n",
    "    \n",
    "    feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "    feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "    feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "    feature_selection_df.head(100)\n",
    "    print(feature_selection_df.columns)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureselection(trainSdImIn12lboheLogSQ,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LicenseNo</th>\n",
       "      <th>FacilityID</th>\n",
       "      <th>FacilityName</th>\n",
       "      <th>Type</th>\n",
       "      <th>Street</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Reason</th>\n",
       "      <th>SectionViolations</th>\n",
       "      <th>Geo_Loc</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>Street_Geo_Loc</th>\n",
       "      <th>LocationID_Geo_Loc</th>\n",
       "      <th>City_id-11235901</th>\n",
       "      <th>City_id-11275913</th>\n",
       "      <th>State_id_1890134</th>\n",
       "      <th>State_id_1890135</th>\n",
       "      <th>RiskLevel_High</th>\n",
       "      <th>RiskLevel_Low</th>\n",
       "      <th>RiskLevel_Medium</th>\n",
       "      <th>RiskLevel_Uncertain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4744</td>\n",
       "      <td>8123</td>\n",
       "      <td>7715</td>\n",
       "      <td>326</td>\n",
       "      <td>15522</td>\n",
       "      <td>81876.0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16406</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5903</td>\n",
       "      <td>9368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2973</td>\n",
       "      <td>12268</td>\n",
       "      <td>11664</td>\n",
       "      <td>182</td>\n",
       "      <td>3057</td>\n",
       "      <td>81862.0</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>878</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9837</td>\n",
       "      <td>4843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18223</td>\n",
       "      <td>1112</td>\n",
       "      <td>969</td>\n",
       "      <td>326</td>\n",
       "      <td>14988</td>\n",
       "      <td>81883.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3368</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5333</td>\n",
       "      <td>10799</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20825</td>\n",
       "      <td>20007</td>\n",
       "      <td>19115</td>\n",
       "      <td>326</td>\n",
       "      <td>3661</td>\n",
       "      <td>81859.0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>11839</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10484</td>\n",
       "      <td>3907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2136</td>\n",
       "      <td>16867</td>\n",
       "      <td>10409</td>\n",
       "      <td>326</td>\n",
       "      <td>7876</td>\n",
       "      <td>81886.0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>12264</td>\n",
       "      <td>12.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15023</td>\n",
       "      <td>11870</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147438</th>\n",
       "      <td>23001</td>\n",
       "      <td>19617</td>\n",
       "      <td>18736</td>\n",
       "      <td>182</td>\n",
       "      <td>6229</td>\n",
       "      <td>81873.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>203</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13264</td>\n",
       "      <td>8409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147439</th>\n",
       "      <td>35329</td>\n",
       "      <td>21729</td>\n",
       "      <td>20760</td>\n",
       "      <td>326</td>\n",
       "      <td>3697</td>\n",
       "      <td>81848.0</td>\n",
       "      <td>11</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7202</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10522</td>\n",
       "      <td>338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147440</th>\n",
       "      <td>5361</td>\n",
       "      <td>12685</td>\n",
       "      <td>12038</td>\n",
       "      <td>326</td>\n",
       "      <td>13027</td>\n",
       "      <td>81877.0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3614</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3245</td>\n",
       "      <td>9485</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147441</th>\n",
       "      <td>12338</td>\n",
       "      <td>10898</td>\n",
       "      <td>10324</td>\n",
       "      <td>326</td>\n",
       "      <td>17833</td>\n",
       "      <td>81888.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>757</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8407</td>\n",
       "      <td>12539</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147442</th>\n",
       "      <td>18743</td>\n",
       "      <td>1392</td>\n",
       "      <td>1249</td>\n",
       "      <td>75</td>\n",
       "      <td>7750</td>\n",
       "      <td>81904.0</td>\n",
       "      <td>6</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16090</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14888</td>\n",
       "      <td>15808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147443 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        LicenseNo  FacilityID  FacilityName  Type  Street  LocationID  Reason  \\\n",
       "0            4744        8123          7715   326   15522     81876.0       0   \n",
       "1            2973       12268         11664   182    3057     81862.0       2   \n",
       "2           18223        1112           969   326   14988     81883.0       0   \n",
       "3           20825       20007         19115   326    3661     81859.0       1   \n",
       "4            2136       16867         10409   326    7876     81886.0       2   \n",
       "...           ...         ...           ...   ...     ...         ...     ...   \n",
       "147438      23001       19617         18736   182    6229     81873.0       2   \n",
       "147439      35329       21729         20760   326    3697     81848.0      11   \n",
       "147440       5361       12685         12038   326   13027     81877.0       0   \n",
       "147441      12338       10898         10324   326   17833     81888.0       1   \n",
       "147442      18743        1392          1249    75    7750     81904.0       6   \n",
       "\n",
       "        SectionViolations  Geo_Loc   day  ...  Street_Geo_Loc  \\\n",
       "0                    33.0    16406  26.0  ...            5903   \n",
       "1                    33.0      878  21.0  ...            9837   \n",
       "2                    32.0     3368   5.0  ...            5333   \n",
       "3                    31.0    11839  28.0  ...           10484   \n",
       "4                    30.0    12264  12.0  ...           15023   \n",
       "...                   ...      ...   ...  ...             ...   \n",
       "147438                3.0      203  15.0  ...           13264   \n",
       "147439               32.0     7202  28.0  ...           10522   \n",
       "147440               32.0     3614  23.0  ...            3245   \n",
       "147441                3.0      757  25.0  ...            8407   \n",
       "147442               32.0    16090  14.0  ...           14888   \n",
       "\n",
       "        LocationID_Geo_Loc  City_id-11235901  City_id-11275913  \\\n",
       "0                     9368               1.0               0.0   \n",
       "1                     4843               1.0               0.0   \n",
       "2                    10799               1.0               0.0   \n",
       "3                     3907               1.0               0.0   \n",
       "4                    11870               1.0               0.0   \n",
       "...                    ...               ...               ...   \n",
       "147438                8409               1.0               0.0   \n",
       "147439                 338               1.0               0.0   \n",
       "147440                9485               1.0               0.0   \n",
       "147441               12539               1.0               0.0   \n",
       "147442               15808               1.0               0.0   \n",
       "\n",
       "        State_id_1890134  State_id_1890135  RiskLevel_High  RiskLevel_Low  \\\n",
       "0                    1.0               0.0             1.0            0.0   \n",
       "1                    1.0               0.0             1.0            0.0   \n",
       "2                    1.0               0.0             1.0            0.0   \n",
       "3                    1.0               0.0             0.0            0.0   \n",
       "4                    1.0               0.0             1.0            0.0   \n",
       "...                  ...               ...             ...            ...   \n",
       "147438               1.0               0.0             0.0            0.0   \n",
       "147439               1.0               0.0             1.0            0.0   \n",
       "147440               1.0               0.0             0.0            0.0   \n",
       "147441               1.0               0.0             1.0            0.0   \n",
       "147442               1.0               0.0             1.0            0.0   \n",
       "\n",
       "        RiskLevel_Medium  RiskLevel_Uncertain  \n",
       "0                    0.0                  0.0  \n",
       "1                    0.0                  0.0  \n",
       "2                    0.0                  0.0  \n",
       "3                    1.0                  0.0  \n",
       "4                    0.0                  0.0  \n",
       "...                  ...                  ...  \n",
       "147438               1.0                  0.0  \n",
       "147439               0.0                  0.0  \n",
       "147440               1.0                  0.0  \n",
       "147441               0.0                  0.0  \n",
       "147442               0.0                  0.0  \n",
       "\n",
       "[147443 rows x 33 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSdImIn12lboheD #,testSdImIn12lboheD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "clf1 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=None, max_features=33, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=8, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "clf2 = BaggingClassifier(base_estimator=clf1, n_estimators=100, \n",
    "                             bootstrap=True,\n",
    "                             bootstrap_features=False, oob_score=True, warm_start=False,\n",
    "                             n_jobs=-1, random_state=786, verbose=0)\n",
    "\n",
    "# clf3=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.8, gamma=0.5,\n",
    "#               learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
    "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
    "#               nthread=None, objective='multi:softprob', random_state=0,\n",
    "#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#               silent=None, subsample=0.6, verbosity=1)\n",
    "\n",
    "\n",
    "clf4 = LGBMClassifier(n_estimators=5000, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5=LGBMClassifier(n_estimators = 10000, \n",
    "    learning_rate= 0.015,\n",
    "    boosting_type = 'gbdt', \n",
    "    colsample_bytree= 0.80, \n",
    "    min_child_weight= 10.0, \n",
    "    num_leaves = 64, \n",
    "    objective='multiclass', \n",
    "    num_class= 7,\n",
    "    subsample = 0.70, \n",
    "    subsample_freq= 5,     \n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf6=LGBMClassifier(n_estimators = 15000, \n",
    "    learning_rate= 0.01,\n",
    "    boosting_type = 'gbdt', \n",
    "    colsample_bytree= 0.80, \n",
    "    min_child_weight= 9.0, \n",
    "    num_leaves = 64, \n",
    "    objective='multiclass', \n",
    "    num_class= 7,\n",
    "    subsample = 0.85, \n",
    "    subsample_freq= 6,     \n",
    "    n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4.fit(trainSdImIn12lboheD,y)\n",
    "pred4 = clf4.predict_proba(testSdImIn12lboheD)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf5.fit(trainSdImIn12lboheD,y)\n",
    "pred5 = clf5.predict_proba(testSdImIn12lboheD)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf6.fit(trainSdImIn12lboheD,y)\n",
    "pred6 = clf6.predict_proba(testSdImIn12lboheD)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub6 = pd.DataFrame(data=np.mean([pred4,pred5,pred6], axis=0)\n",
    "                         , columns=submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub6.to_excel(excel_writer = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/sub6.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.fit(trainSdImIn12lboheD,y)\n",
    "\n",
    "pred1 = clf2.predict_proba(testSdImIn12lboheD)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf3.fit(trainSdImIn12lboheLogSQ,y)\n",
    "\n",
    "# pred2 = clf3.predict_proba(testSdImIn12lboheLogSQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4.fit(trainSdImIn12lboheD,y)\n",
    "\n",
    "pred3 = clf4.predict_proba(testSdImIn12lboheD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_sub1 = pd.DataFrame(data=np.mean([pred1, pred2,pred3], axis=0)\n",
    "#                          , columns=submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub3 = pd.DataFrame(data= pred1\n",
    "                         , columns=submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub4 = pd.DataFrame(data=np.mean([pred1,pred3], axis=0)\n",
    "                         , columns=submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub5 = pd.DataFrame(data = pred3\n",
    "                         , columns=submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub3.to_excel(excel_writer = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/sub3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xt-Op3_XDPCI"
   },
   "outputs": [],
   "source": [
    "final_sub4.to_excel(excel_writer = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/sub4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub5.to_excel(excel_writer = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/sub5.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "awaybasic-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
