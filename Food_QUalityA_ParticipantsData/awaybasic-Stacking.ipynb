{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import category_encoders\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import matplotlib as plt\n",
    "from random import randint\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import matplotlib as plt\n",
    "from random import randint\n",
    "\n",
    "\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file into a variable fifa_data\n",
    "filepath = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/Data_Train.xlsx\"\n",
    "data = pd.read_excel(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdata = pd.read_excel('/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/Data_Test.xlsx')\n",
    "# data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "y = data.Inspection_Results\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "X = data.drop(['Inspection_Results'], axis=1)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.65, test_size=0.35,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>LicenseNo</th>\n",
       "      <th>FacilityID</th>\n",
       "      <th>FacilityName</th>\n",
       "      <th>Type</th>\n",
       "      <th>Street</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Reason</th>\n",
       "      <th>SectionViolations</th>\n",
       "      <th>RiskLevel</th>\n",
       "      <th>Geo_Loc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76851</th>\n",
       "      <td>24443412747647</td>\n",
       "      <td>09-07-2018</td>\n",
       "      <td>28225</td>\n",
       "      <td>4702</td>\n",
       "      <td>4480</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>13072</td>\n",
       "      <td>id-11235901</td>\n",
       "      <td>id_1890134</td>\n",
       "      <td>81875.0</td>\n",
       "      <td>CANVASS</td>\n",
       "      <td>2.0</td>\n",
       "      <td>High</td>\n",
       "      <td>locid15302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36203</th>\n",
       "      <td>47254212658879</td>\n",
       "      <td>17-11-2014</td>\n",
       "      <td>743</td>\n",
       "      <td>25793</td>\n",
       "      <td>24635</td>\n",
       "      <td>CAFE/STORE</td>\n",
       "      <td>2087</td>\n",
       "      <td>id-11235901</td>\n",
       "      <td>id_1890134</td>\n",
       "      <td>81882.0</td>\n",
       "      <td>CANVASS</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Medium</td>\n",
       "      <td>locid3211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>25946353205233</td>\n",
       "      <td>19-12-2015</td>\n",
       "      <td>24297</td>\n",
       "      <td>13257</td>\n",
       "      <td>12588</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>2382</td>\n",
       "      <td>id-11235901</td>\n",
       "      <td>id_1890134</td>\n",
       "      <td>81855.0</td>\n",
       "      <td>CANVASS RE-INSPECTION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>High</td>\n",
       "      <td>locid9774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51377</th>\n",
       "      <td>28473392743625</td>\n",
       "      <td>21-10-2010</td>\n",
       "      <td>18030</td>\n",
       "      <td>7609</td>\n",
       "      <td>7234</td>\n",
       "      <td>SCHOOL</td>\n",
       "      <td>12441</td>\n",
       "      <td>id-11235901</td>\n",
       "      <td>id_1890134</td>\n",
       "      <td>81883.0</td>\n",
       "      <td>CANVASS</td>\n",
       "      <td>32.0</td>\n",
       "      <td>High</td>\n",
       "      <td>locid3875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116892</th>\n",
       "      <td>45067242875655</td>\n",
       "      <td>31-01-2012</td>\n",
       "      <td>17544</td>\n",
       "      <td>23678</td>\n",
       "      <td>22588</td>\n",
       "      <td>RESTAURANT</td>\n",
       "      <td>479</td>\n",
       "      <td>id-11235901</td>\n",
       "      <td>id_1890134</td>\n",
       "      <td>81862.0</td>\n",
       "      <td>CANVASS</td>\n",
       "      <td>33.0</td>\n",
       "      <td>High</td>\n",
       "      <td>locid511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID        Date  LicenseNo  FacilityID  FacilityName  \\\n",
       "76851   24443412747647  09-07-2018      28225        4702          4480   \n",
       "36203   47254212658879  17-11-2014        743       25793         24635   \n",
       "3760    25946353205233  19-12-2015      24297       13257         12588   \n",
       "51377   28473392743625  21-10-2010      18030        7609          7234   \n",
       "116892  45067242875655  31-01-2012      17544       23678         22588   \n",
       "\n",
       "              Type  Street         City       State  LocationID  \\\n",
       "76851   RESTAURANT   13072  id-11235901  id_1890134     81875.0   \n",
       "36203   CAFE/STORE    2087  id-11235901  id_1890134     81882.0   \n",
       "3760    RESTAURANT    2382  id-11235901  id_1890134     81855.0   \n",
       "51377       SCHOOL   12441  id-11235901  id_1890134     81883.0   \n",
       "116892  RESTAURANT     479  id-11235901  id_1890134     81862.0   \n",
       "\n",
       "                       Reason  SectionViolations RiskLevel     Geo_Loc  \n",
       "76851                 CANVASS                2.0      High  locid15302  \n",
       "36203                 CANVASS               32.0    Medium   locid3211  \n",
       "3760    CANVASS RE-INSPECTION                NaN      High   locid9774  \n",
       "51377                 CANVASS               32.0      High   locid3875  \n",
       "116892                CANVASS               33.0      High    locid511  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "              colsample_bynode=1, colsample_bytree=0.8, gamma=0.5,\n",
    "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
    "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
    "              nthread=None, objective='multi:softprob', random_state=0,\n",
    "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "              silent=None, subsample=0.6, verbosity=1)\n",
    "\n",
    "clf2 = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
    "                       criterion='entropy', max_depth=8, max_features='auto',\n",
    "                       max_leaf_nodes=None, max_samples=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=1, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
    "                       n_jobs=None, oob_score=False, random_state=None,\n",
    "                       verbose=0, warm_start=False)\n",
    "\n",
    "clf3= LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
    "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "\n",
    "clf4 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=None, max_features=11, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=8, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "clf5 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=1.0, max_features=1.0, \n",
    "                         bootstrap=True,\n",
    "                         bootstrap_features=False, oob_score=True, warm_start=False,\n",
    "                         n_jobs=-1, random_state=None, verbose=0)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropcol(train,test):\n",
    "    train['logID'] = np.log(train['ID'])\n",
    "    test['logID'] = np.log(test['ID'])\n",
    "    cols_to_drop = ['Date','ID']\n",
    "    dtrain = train.drop(cols_to_drop,axis =1)\n",
    "    dtest = test.drop(cols_to_drop,axis =1)\n",
    "    \n",
    "    print(dtrain.shape,dtest.shape)\n",
    "\n",
    "    return dtrain,dtest\n",
    "\n",
    "def catVar1(data):\n",
    "    categorical_colsT1 = [cname for cname in data.columns if\n",
    "                    data[cname].nunique() <=10 and \n",
    "                    data[cname].dtype == \"object\"]\n",
    "    return categorical_colsT1\n",
    "\n",
    "def catVar2(data):\n",
    "    categorical_colsT2 = [cname for cname in data.columns if\n",
    "                    data[cname].nunique() >1 and \n",
    "                    data[cname].dtype == \"object\"]\n",
    "    return categorical_colsT2\n",
    "\n",
    "def NumVar(data) :\n",
    "    numerical_cols = [cname for cname in data.columns if \n",
    "    data[cname].dtype in ['int64', 'float64']]\n",
    "    return numerical_cols\n",
    "\n",
    "def imputer(train,test):\n",
    "    \n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    numerical_cols = NumVar(train)\n",
    "    my_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    imputed_train = pd.DataFrame(my_imputer.fit_transform(train))\n",
    "    imputed_test = pd.DataFrame(my_imputer.transform(test))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_train.columns = train.columns\n",
    "    imputed_test.columns = test.columns\n",
    "    \n",
    "    #restoring datatypes \n",
    "    imputed_train[numerical_cols] = imputed_train[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    imputed_test[numerical_cols] = imputed_test[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    print(imputed_train.shape,imputed_test.shape)\n",
    "    return imputed_train,imputed_test \n",
    "\n",
    "def robustlabelencoder(train,test):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    label_enc = LabelEncoderExt()\n",
    "    cols = catVar2(train)\n",
    "    print(cols)\n",
    "    for col in cols:\n",
    "        label_enc.fit(train[col])\n",
    "        train[col] = label_enc.transform(train[col])\n",
    "        test[col] = label_enc.transform(test[col])\n",
    "        \n",
    "    print(train.shape,test.shape)\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "def normalabelencoder(train,test,cols):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    label_enc = LabelEncoder()\n",
    "    \n",
    "    print(cols)\n",
    "    for col in cols:\n",
    "        label_enc.fit(train[col])\n",
    "        train[col] = label_enc.transform(train[col])\n",
    "        test[col] = label_enc.transform(test[col])\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "\n",
    "\n",
    "def ohebygetdummis(train,test):\n",
    "    pass\n",
    "\n",
    "def onhotencoder(train,test):\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    object_cols = catVar1(train) #catVar1 gives desired categorical column and not all object columns\n",
    "    print(object_cols)\n",
    "    \n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[object_cols]))\n",
    "    OH_cols_test = pd.DataFrame(OH_encoder.transform(test[object_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = train.index\n",
    "    OH_cols_test.index = test.index\n",
    "\n",
    "    # Remove desired categorical columns (will replace with one-hot encoding)\n",
    "    num_train = train.drop(object_cols, axis=1)\n",
    "    num_test = test.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical/remaining features\n",
    "    OH_train = pd.concat([num_train, OH_cols_train], axis=1)\n",
    "    OH_test = pd.concat([num_test, OH_cols_test], axis=1)\n",
    "    \n",
    "    print(OH_train.shape,OH_test.shape)\n",
    "    \n",
    "    \n",
    "    return OH_train,OH_test\n",
    "\n",
    "def missingcheck(data):\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    percent_1 = data.isnull().sum()/data.isnull().count()*100\n",
    "    percent_2 = (np.round(percent_1, 1)).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%']) #ptr\n",
    "    return missing_data\n",
    "\n",
    "def targetencoding(train,test,y_train):\n",
    "    import category_encoders as ce\n",
    "    # Create the encoder itself\n",
    "    cat_features = catVar2(train)\n",
    "    print(f'targest emcoding for features {cat_features}')\n",
    "    target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "    \n",
    "\n",
    "    # Fit the encoder using the categorical features and target\n",
    "    target_enc.fit(train[cat_features], y_train)\n",
    "    \n",
    "\n",
    "    # Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "    traintrgtenc = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "    testtrgtenc = test.join(target_enc.transform(test[cat_features]).add_suffix('_target'))\n",
    "    \n",
    "    print(traintrgtenc.shape,testtrgtenc.shape)\n",
    "    \n",
    "    return traintrgtenc,testtrgtenc\n",
    "\n",
    "\n",
    "def special(train,test):\n",
    "    train['Geo_Loc'] = train['Geo_Loc'].str.replace(r'\\D', '')\n",
    "    train['Geo_Loc'] = pd.to_numeric(train['Geo_Loc'], errors='coerce') \n",
    "    \n",
    "    test['Geo_Loc'] = test['Geo_Loc'].str.replace(r'\\D', '')\n",
    "    test['Geo_Loc'] = pd.to_numeric(test['Geo_Loc'], errors='coerce')\n",
    "    \n",
    "    train['Date'] = pd.to_datetime(train['Date'] ,errors='coerce')\n",
    "    test['Date'] = pd.to_datetime(test['Date'] ,errors='coerce')\n",
    "    \n",
    "    train = train.assign(\n",
    "               hour=train.Date.dt.hour,\n",
    "               day=train.Date.dt.day,\n",
    "               month=train.Date.dt.month,\n",
    "               year=train.Date.dt.year\n",
    "                        )\n",
    "    \n",
    "    test = test.assign(\n",
    "               hour=test.Date.dt.hour,\n",
    "               day=test.Date.dt.day,\n",
    "               month=test.Date.dt.month,\n",
    "               year=test.Date.dt.year\n",
    "                        )\n",
    "    print(train.shape,test.shape)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def syntheticvariable(train , y ):\n",
    "#     from imblearn.over_sampling import SMOTENC\n",
    "#     from collections import Counter\n",
    "#     categorical_features = [cname.index for cname in train.columns if train[cname].dtype == \"object\"]\n",
    "#     out = np.argwhere(train.columns.isin(categorical_features)).ravel().tolist()\n",
    "    \n",
    "#     smote_nc = SMOTENC([3, 5, 6, 8, 10], random_state=0)  #instead of passing out i need to hardcode it.\n",
    "\n",
    "#     X_resampled, y_resampled = smote_nc.fit_resample(train,y)\n",
    "    \n",
    "#     print(sorted(Counter(y_resampled).items()))\n",
    "    \n",
    "#     return X_resampled,y_resampled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####crete new features###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createIntercations(train,test,cat_features):\n",
    "    import itertools\n",
    "    print(f\"Creating features on {cat_features}, with combination 2 for training data /n\")\n",
    "    interactionstrain = pd.DataFrame(index=train.index)\n",
    "    \n",
    "    for col1 ,col2 in  itertools.combinations(cat_features,2):   \n",
    "        newcolname = col1 + \"_\" + col2 \n",
    "        new_values = train[col1].map(str) + \"_\" + train[col2].map(str)\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "    \n",
    "    train_df = train.join(interactionstrain)\n",
    "    \n",
    "    print(f\"Creating features on {cat_features}, with combination 2 for testing data\")\n",
    "    interactionstest = pd.DataFrame(index=train.index)\n",
    "    \n",
    "    for col1 ,col2 in  itertools.combinations(cat_features,2):   \n",
    "        newcolname = col1 + \"_\" + col2 \n",
    "        new_values = test[col1].map(str) + \"_\" + test[col2].map(str)\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "    test_df = test.join(interactionstest)\n",
    "    \n",
    "    print(train_df.shape,test_df.shape)\n",
    "    \n",
    "    \n",
    "    return train_df,test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "    \n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    #model = RandomForestClassifier(random_state=0)\n",
    "    #model.fit(X_train, y_train)\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "    clf1 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=None, max_features=25, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=8, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "    clf2 = BaggingClassifier(base_estimator=clf1, n_estimators=100, \n",
    "                             bootstrap=True,\n",
    "                             bootstrap_features=False, oob_score=True, warm_start=False,\n",
    "                             n_jobs=-1, random_state=786, verbose=0)\n",
    "    clf2.fit(X_train,y_train)\n",
    "\n",
    "    preds = clf2.predict(X_valid)\n",
    "    target_names = ['class 0', 'class 1', 'class 2','class 3', 'class 4', 'class 5', 'class 6']\n",
    "    print(classification_report(y_valid, preds, target_names=target_names,labels= [0,1,2,3,4,5,6]))\n",
    "    return clf2.score(X_valid,y_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a17c4e3a2682>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# X_train, X_valid, y_train, y_valid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf1' is not defined"
     ]
    }
   ],
   "source": [
    "# X_train, X_valid, y_train, y_valid\n",
    "clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/.local/lib/python3.6/site-packages/ipykernel_launcher.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ryan/.local/lib/python3.6/site-packages/ipykernel_launcher.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ryan/.local/lib/python3.6/site-packages/ipykernel_launcher.py:141: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ryan/.local/lib/python3.6/site-packages/ipykernel_launcher.py:142: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/ryan/.local/lib/python3.6/site-packages/ipykernel_launcher.py:144: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95837, 18) (51606, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/.local/lib/python3.6/site-packages/ipykernel_launcher.py:145: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "X_trainS, X_validS = special(X_train ,X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95837, 17) (51606, 17)\n"
     ]
    }
   ],
   "source": [
    "X_trainSDr, X_validSDr = dropcol(X_trainS,X_validS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95837, 17) (51606, 17)\n"
     ]
    }
   ],
   "source": [
    "X_trainSDrIM, X_validSDrIM = imputer(X_trainSDr,X_validSDr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                   Total    %\n",
       " logID                  0  0.0\n",
       " LocationID             0  0.0\n",
       " FacilityID             0  0.0\n",
       " FacilityName           0  0.0\n",
       " Type                   0  0.0\n",
       " Street                 0  0.0\n",
       " City                   0  0.0\n",
       " State                  0  0.0\n",
       " Reason                 0  0.0\n",
       " year                   0  0.0\n",
       " SectionViolations      0  0.0\n",
       " RiskLevel              0  0.0\n",
       " Geo_Loc                0  0.0\n",
       " hour                   0  0.0\n",
       " day                    0  0.0\n",
       " month                  0  0.0\n",
       " LicenseNo              0  0.0,                    Total    %\n",
       " logID                  0  0.0\n",
       " LocationID             0  0.0\n",
       " FacilityID             0  0.0\n",
       " FacilityName           0  0.0\n",
       " Type                   0  0.0\n",
       " Street                 0  0.0\n",
       " City                   0  0.0\n",
       " State                  0  0.0\n",
       " Reason                 0  0.0\n",
       " year                   0  0.0\n",
       " SectionViolations      0  0.0\n",
       " RiskLevel              0  0.0\n",
       " Geo_Loc                0  0.0\n",
       " hour                   0  0.0\n",
       " day                    0  0.0\n",
       " month                  0  0.0\n",
       " LicenseNo              0  0.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingcheck(X_trainSDrIM),missingcheck( X_validSDrIM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features on ['SectionViolations', 'RiskLevel', 'Reason'], with combination 2 for training data /n\n",
      "Creating features on ['SectionViolations', 'RiskLevel', 'Reason'], with combination 2 for testing data\n",
      "(95837, 20) (51606, 20)\n"
     ]
    }
   ],
   "source": [
    "X_trainSDrIMinter1,X_validSDrIMinter1 = createIntercations(X_trainSDrIM, X_validSDrIM,cat_features = ['SectionViolations','RiskLevel','Reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features on ['State', 'City', 'Street', 'LocationID', 'Geo_Loc'], with combination 2 for training data /n\n",
      "Creating features on ['State', 'City', 'Street', 'LocationID', 'Geo_Loc'], with combination 2 for testing data\n",
      "(95837, 30) (51606, 30)\n"
     ]
    }
   ],
   "source": [
    "X_trainSDrIMinter2,X_validSDrIMinter2 = createIntercations(X_trainSDrIMinter1, X_validSDrIMinter1,cat_features = ['State','City','Street','LocationID','Geo_Loc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targest emcoding for features ['Type', 'City', 'State', 'Reason', 'RiskLevel', 'SectionViolations_RiskLevel', 'SectionViolations_Reason', 'RiskLevel_Reason', 'State_City', 'State_Street', 'State_LocationID', 'State_Geo_Loc', 'City_Street', 'City_LocationID', 'City_Geo_Loc', 'Street_LocationID', 'Street_Geo_Loc', 'LocationID_Geo_Loc']\n",
      "(95837, 48) (51606, 48)\n"
     ]
    }
   ],
   "source": [
    "X_trainSDrIMintertgenc,X_validSDrIMinter2tgenc = targetencoding(X_trainSDrIMinter2,X_validSDrIMinter2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'City', 'State', 'Reason', 'RiskLevel', 'SectionViolations_RiskLevel', 'SectionViolations_Reason', 'RiskLevel_Reason', 'State_City', 'State_Street', 'State_LocationID', 'State_Geo_Loc', 'City_Street', 'City_LocationID', 'City_Geo_Loc', 'Street_LocationID', 'Street_Geo_Loc', 'LocationID_Geo_Loc']\n",
      "(95837, 48) (51606, 48)\n"
     ]
    }
   ],
   "source": [
    "X_trainSDrIMinter2lbl,X_validSDrIMinter2lbl = robustlabelencoder(X_trainSDrIMintertgenc,X_validSDrIMinter2tgenc )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['City', 'State', 'RiskLevel', 'State_City']\n",
      "(95837, 52) (51606, 52)\n"
     ]
    }
   ],
   "source": [
    "# X_trainSDrIMinter2lblohe,X_validSDrIMinter2lblohe = onhotencoder(X_trainSDrIMinter2lbl,X_validSDrIMinter2lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95837, 48) (51606, 48)\n"
     ]
    }
   ],
   "source": [
    "#X_trainSDrIMinter2lblohe.head(2)\n",
    "X_trainSDrIMinter2lbloheqq,X_validSDrIMinter2lbloheqq = imputer(X_trainSDrIMinter2lbl,X_validSDrIMinter2lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95837, 48)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainSDrIMinter2lbloheqq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95837,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51606, 48)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validSDrIMinter2lbloheqq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51606,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LocationID_Geo_Loc_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Street_Geo_Loc_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Street</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_City</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RiskLevel_Reason</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SectionViolations_Reason</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SectionViolations_RiskLevel</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Geo_Loc</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RiskLevel</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SectionViolations</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reason</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LocationID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Street</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FacilityName</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FacilityID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_LocationID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Geo_Loc</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City_Street</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SectionViolations_Reason_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Street_LocationID_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City_Geo_Loc_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City_LocationID_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City_Street_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Geo_Loc_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_LocationID_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_Street_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_City_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RiskLevel_Reason_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SectionViolations_RiskLevel_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City_LocationID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RiskLevel_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reason_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type_target</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LocationID_Geo_Loc</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Street_Geo_Loc</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Street_LocationID</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City_Geo_Loc</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LicenseNo</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Total    %\n",
       "LocationID_Geo_Loc_target               0  0.0\n",
       "Street_Geo_Loc_target                   0  0.0\n",
       "State_Street                            0  0.0\n",
       "State_City                              0  0.0\n",
       "RiskLevel_Reason                        0  0.0\n",
       "SectionViolations_Reason                0  0.0\n",
       "SectionViolations_RiskLevel             0  0.0\n",
       "logID                                   0  0.0\n",
       "year                                    0  0.0\n",
       "month                                   0  0.0\n",
       "day                                     0  0.0\n",
       "hour                                    0  0.0\n",
       "Geo_Loc                                 0  0.0\n",
       "RiskLevel                               0  0.0\n",
       "SectionViolations                       0  0.0\n",
       "Reason                                  0  0.0\n",
       "LocationID                              0  0.0\n",
       "State                                   0  0.0\n",
       "City                                    0  0.0\n",
       "Street                                  0  0.0\n",
       "Type                                    0  0.0\n",
       "FacilityName                            0  0.0\n",
       "FacilityID                              0  0.0\n",
       "State_LocationID                        0  0.0\n",
       "State_Geo_Loc                           0  0.0\n",
       "City_Street                             0  0.0\n",
       "SectionViolations_Reason_target         0  0.0\n",
       "Street_LocationID_target                0  0.0\n",
       "City_Geo_Loc_target                     0  0.0\n",
       "City_LocationID_target                  0  0.0\n",
       "City_Street_target                      0  0.0\n",
       "State_Geo_Loc_target                    0  0.0\n",
       "State_LocationID_target                 0  0.0\n",
       "State_Street_target                     0  0.0\n",
       "State_City_target                       0  0.0\n",
       "RiskLevel_Reason_target                 0  0.0\n",
       "SectionViolations_RiskLevel_target      0  0.0\n",
       "City_LocationID                         0  0.0\n",
       "RiskLevel_target                        0  0.0\n",
       "Reason_target                           0  0.0\n",
       "State_target                            0  0.0\n",
       "City_target                             0  0.0\n",
       "Type_target                             0  0.0\n",
       "LocationID_Geo_Loc                      0  0.0\n",
       "Street_Geo_Loc                          0  0.0\n",
       "Street_LocationID                       0  0.0\n",
       "City_Geo_Loc                            0  0.0\n",
       "LicenseNo                               0  0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingcheck(X_trainSDrIMinter2lbloheqq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LicenseNo', 'FacilityID', 'FacilityName', 'Type', 'Street', 'City',\n",
       "       'State', 'LocationID', 'Reason', 'SectionViolations', 'RiskLevel',\n",
       "       'Geo_Loc', 'hour', 'day', 'month', 'year', 'logID',\n",
       "       'SectionViolations_RiskLevel', 'SectionViolations_Reason',\n",
       "       'RiskLevel_Reason', 'State_City', 'State_Street', 'State_LocationID',\n",
       "       'State_Geo_Loc', 'City_Street', 'City_LocationID', 'City_Geo_Loc',\n",
       "       'Street_LocationID', 'Street_Geo_Loc', 'LocationID_Geo_Loc',\n",
       "       'Type_target', 'City_target', 'State_target', 'Reason_target',\n",
       "       'RiskLevel_target', 'SectionViolations_RiskLevel_target',\n",
       "       'SectionViolations_Reason_target', 'RiskLevel_Reason_target',\n",
       "       'State_City_target', 'State_Street_target', 'State_LocationID_target',\n",
       "       'State_Geo_Loc_target', 'City_Street_target', 'City_LocationID_target',\n",
       "       'City_Geo_Loc_target', 'Street_LocationID_target',\n",
       "       'Street_Geo_Loc_target', 'LocationID_Geo_Loc_target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainSDrIMinter2lbloheqq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_to_drop = []\n",
    "\n",
    "# dtrain = X_trainSDrIMinter2lbloheqq.drop(cols_to_drop,axis =1)\n",
    "# dtest = X_validSDrIMinter2lbloheqq.drop(cols_to_drop,axis =1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# from sklearn.feature_selection import SelectFromModel\n",
    "# extclf = ExtraTreesClassifier(n_estimators=100, random_state=7)\n",
    "\n",
    "# # Set the regularization parameter C=1\n",
    "# logistic = RandomForestClassifier(n_estimators=100, random_state=7 ).fit(X_trainSDrIMinter2lbloheqq, y_train)\n",
    "# model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "\n",
    "# X_new = model.transform(X_trainSDrIMinter2lbloheqq)\n",
    "# X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "#                                  index=X_trainSDrIMinter2lbloheqq.index,\n",
    "#                                  columns=X_trainSDrIMinter2lbloheqq.columns)\n",
    "\n",
    "# # Dropped columns have values of all 0s, keep other columns \n",
    "# selected_columns = selected_features.columns[selected_features.var() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# selector = SelectKBest(f_classif, k=20)\n",
    "\n",
    "# X_new1 = selector.fit_transform(X_trainSDrIMinter2lbloheqq, y_train)\n",
    "# X_new1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features1 = pd.DataFrame(selector.inverse_transform(X_new1), \n",
    "#                                  index=X_trainSDrIMinter2lbloheqq.index,\n",
    "#                                  columns=X_trainSDrIMinter2lbloheqq.columns)\n",
    "\n",
    "# # Dropped columns have values of all 0s, keep other columns \n",
    "# selected_columns1 = selected_features1.columns[selected_features1.var() != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_columns1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_trainSDrIMinter2lbloheqq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-281ce4915a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainSDrIMinter2lbloheqq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_validSDrIMinter2lbloheqq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_trainSDrIMinter2lbloheqq' is not defined"
     ]
    }
   ],
   "source": [
    "score_dataset(X_trainSDrIMinter2lbloheqq,X_validSDrIMinter2lbloheqq,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Reason']\n"
     ]
    }
   ],
   "source": [
    "X_trainSDrIMLbl, X_validSDrIMLbl = lblencdr(X_trainSDrIM, X_validSDrIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['City', 'State', 'RiskLevel']\n"
     ]
    }
   ],
   "source": [
    "X_trainSDrIMLblOhe, X_validSDrIMLblOhe = onhotencoder(X_trainSDrIMLbl, X_validSDrIMLbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.00      0.00      0.00        17\n",
      "     class 1       0.79      0.70      0.74     10009\n",
      "     class 2       0.56      0.30      0.39       465\n",
      "     class 3       0.49      0.18      0.26      1641\n",
      "     class 4       0.87      0.96      0.91     27842\n",
      "     class 5       0.72      0.73      0.72      7138\n",
      "     class 6       0.71      0.60      0.65      4494\n",
      "\n",
      "    accuracy                           0.81     51606\n",
      "   macro avg       0.59      0.50      0.53     51606\n",
      "weighted avg       0.80      0.81      0.80     51606\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.813839476029919"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dataset(X_trainSDrIMLblOhe,X_validSDrIMLblOhe,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
