{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kFEo8EwDLq1H",
    "outputId": "3a82fbf3-bed9-41fb-f462-205cb544e604"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import category_encoders\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import matplotlib as plt\n",
    "from random import randint\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoz1hAGBLq1Q"
   },
   "outputs": [],
   "source": [
    "# Read the file into a variable fifa_data\n",
    "filepath = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/Data_Train.xlsx\"\n",
    "data = pd.read_excel(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file into a variable fifa_data\n",
    " \n",
    "test = pd.read_excel(\"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/Data_Test.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hbEjJjM0Lq1X"
   },
   "outputs": [],
   "source": [
    "submission= pd.read_excel('/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/Sample_Submission.xlsx')\n",
    "# data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQx71DcSLq1v"
   },
   "outputs": [],
   "source": [
    "class LabelEncoderExt(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n",
    "        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n",
    "        \"\"\"\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        # self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "    def fit(self, data_list):\n",
    "        \"\"\"\n",
    "        This will fit the encoder for all the unique values and introduce unknown value\n",
    "        :param data_list: A list of string\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_list):\n",
    "        \"\"\"\n",
    "        This will transform the data_list to id list where the new values get assigned to Unknown class\n",
    "        :param data_list:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_data_list = list(data_list)\n",
    "        for unique_item in np.unique(data_list):\n",
    "            if unique_item not in self.label_encoder.classes_:\n",
    "                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n",
    "\n",
    "        return self.label_encoder.transform(new_data_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# clf1 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "#                        max_depth=None, max_features=11, max_leaf_nodes=None,\n",
    "#                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                        min_samples_leaf=8, min_samples_split=2,\n",
    "#                        min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "#                        random_state=None, splitter='best')\n",
    "\n",
    "# clf2 = BaggingClassifier(base_estimator=clf1, n_estimators=100, max_samples=1.0, max_features=1.0, \n",
    "#                          bootstrap=True,\n",
    "#                          bootstrap_features=False, oob_score=True, warm_start=False,\n",
    "#                          n_jobs=-1, random_state=None, verbose=0)\n",
    "\n",
    "# clf3=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.8, gamma=0.5,\n",
    "#               learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
    "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
    "#               nthread=None, objective='multi:softprob', random_state=0,\n",
    "#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#               silent=None, subsample=0.6, verbosity=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaOKY4bpLq11"
   },
   "outputs": [],
   "source": [
    "def dropcol(train,test):\n",
    "    train['logID'] = np.log1p(train['ID'])\n",
    "    test['logID'] = np.log1p(test['ID'])\n",
    "    cols_to_drop = ['Date','ID']\n",
    "    dtrain = train.drop(cols_to_drop,axis =1)\n",
    "    dtest = test.drop(cols_to_drop,axis =1)\n",
    "    \n",
    "    print(dtrain.shape,dtest.shape)\n",
    "\n",
    "    return dtrain,dtest\n",
    "\n",
    "def catVar1(data):\n",
    "    categorical_colsT1 = [cname for cname in data.columns if\n",
    "                    data[cname].nunique() <=10 and \n",
    "                    data[cname].dtype == \"object\"]\n",
    "    return categorical_colsT1\n",
    "\n",
    "def catVar2(data):\n",
    "    categorical_colsT2 = [cname for cname in data.columns if\n",
    "                    data[cname].nunique() >10 and \n",
    "                    data[cname].dtype == \"object\"]\n",
    "    return categorical_colsT2\n",
    "\n",
    "def NumVar(data) :\n",
    "    numerical_cols = [cname for cname in data.columns if \n",
    "    data[cname].dtype in ['int64', 'float64']]\n",
    "    return numerical_cols\n",
    "\n",
    "def imputer(train,test):\n",
    "    \n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    numerical_cols = NumVar(train)\n",
    "    my_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    imputed_train = pd.DataFrame(my_imputer.fit_transform(train))\n",
    "    imputed_test = pd.DataFrame(my_imputer.transform(test))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_train.columns = train.columns\n",
    "    imputed_test.columns = test.columns\n",
    "    \n",
    "    #restoring datatypes \n",
    "    imputed_train[numerical_cols] = imputed_train[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    imputed_test[numerical_cols] = imputed_test[numerical_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    print(imputed_train.shape,imputed_test.shape)\n",
    "    return imputed_train,imputed_test \n",
    "\n",
    "def robustlabelencoder(train,test):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    label_enc = LabelEncoderExt()\n",
    "    cols = catVar2(train)\n",
    "    print(cols)\n",
    "    for col in cols:\n",
    "        label_enc.fit(train[col])\n",
    "        train[col] = label_enc.transform(train[col])\n",
    "        test[col] = label_enc.transform(test[col])\n",
    "        \n",
    "    print(train.shape,test.shape)\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "def normalabelencoder(train,test,cols):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    label_enc = LabelEncoder()\n",
    "    \n",
    "    print(cols)\n",
    "    for col in cols:\n",
    "        label_enc.fit(train[col])\n",
    "        train[col] = label_enc.transform(train[col])\n",
    "        test[col] = label_enc.transform(test[col])\n",
    "    \n",
    "    return train,test\n",
    "\n",
    "\n",
    "\n",
    "def ohebygetdummis(train,test):\n",
    "    pass\n",
    "\n",
    "def onhotencoder(train,test):\n",
    "    \n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    object_cols = catVar1(train) #catVar1 gives desired categorical column and not all object columns\n",
    "    print(object_cols)\n",
    "    \n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[object_cols]))\n",
    "    OH_cols_test = pd.DataFrame(OH_encoder.transform(test[object_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = train.index\n",
    "    OH_cols_test.index = test.index\n",
    "\n",
    "    ##hack for restoring columns names just like get dummies\n",
    "    column_name = OH_encoder.get_feature_names(object_cols)\n",
    "    OH_cols_train.columns = column_name\n",
    "    OH_cols_test.columns = column_name\n",
    "    \n",
    "\n",
    "    # Remove desired categorical columns (will replace with one-hot encoding)\n",
    "    num_train = train.drop(object_cols, axis=1)\n",
    "    num_test = test.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical/remaining features\n",
    "    OH_train = pd.concat([num_train, OH_cols_train], axis=1)\n",
    "    OH_test = pd.concat([num_test, OH_cols_test], axis=1)\n",
    "    \n",
    "    print(OH_train.shape,OH_test.shape)\n",
    "    \n",
    "    \n",
    "    return OH_train,OH_test\n",
    "\n",
    "def missingcheck(data):\n",
    "    total = data.isnull().sum().sort_values(ascending=False)\n",
    "    percent_1 = data.isnull().sum()/data.isnull().count()*100\n",
    "    percent_2 = (np.round(percent_1, 1)).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%']) #ptr\n",
    "    return missing_data\n",
    "\n",
    "def targetencoding(train,test,y_train):\n",
    "    import category_encoders as ce\n",
    "    # Create the encoder itself\n",
    "    cat_features = catVar2(train)\n",
    "    print(f'targest emcoding for features {cat_features}')\n",
    "    target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "    \n",
    "\n",
    "    # Fit the encoder using the categorical features and target\n",
    "    target_enc.fit(train[cat_features], y_train)\n",
    "    \n",
    "\n",
    "    # Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "    traintrgtenc = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "    testtrgtenc = test.join(target_enc.transform(test[cat_features]).add_suffix('_target'))\n",
    "\n",
    "    traintrgtenc = traintrgtenc.drop(cat_features, axis =1)\n",
    "    testtrgtenc = testtrgtenc.drop(cat_features, axis =1)\n",
    "\n",
    "    \n",
    "    print(traintrgtenc.shape,testtrgtenc.shape)\n",
    "    \n",
    "    return traintrgtenc,testtrgtenc\n",
    "\n",
    "\n",
    "def special(train,test):\n",
    "    train['Geo_Loc'] = train['Geo_Loc'].str.replace(r'\\D', '')\n",
    "    train['Geo_Loc'] = pd.to_numeric(train['Geo_Loc'], errors='coerce') \n",
    "    \n",
    "    test['Geo_Loc'] = test['Geo_Loc'].str.replace(r'\\D', '')\n",
    "    test['Geo_Loc'] = pd.to_numeric(test['Geo_Loc'], errors='coerce')\n",
    "    \n",
    "    train['Date'] = pd.to_datetime(train['Date'] ,errors='coerce')\n",
    "    test['Date'] = pd.to_datetime(test['Date'] ,errors='coerce')\n",
    "    \n",
    "    train = train.assign(\n",
    "               hour=train.Date.dt.hour,\n",
    "               day=train.Date.dt.day,\n",
    "               month=train.Date.dt.month,\n",
    "               year=train.Date.dt.year\n",
    "                        )\n",
    "    \n",
    "    test = test.assign(\n",
    "               hour=test.Date.dt.hour,\n",
    "               day=test.Date.dt.day,\n",
    "               month=test.Date.dt.month,\n",
    "               year=test.Date.dt.year\n",
    "                        )\n",
    "    print(train.shape,test.shape)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0gqe6UoLq15"
   },
   "outputs": [],
   "source": [
    "# def syntheticvariable(train , y ):\n",
    "#     from imblearn.over_sampling import SMOTENC\n",
    "#     from collections import Counter\n",
    "#     categorical_features = [cname.index for cname in train.columns if train[cname].dtype == \"object\"]\n",
    "#     out = np.argwhere(train.columns.isin(categorical_features)).ravel().tolist()\n",
    "    \n",
    "#     smote_nc = SMOTENC([3, 5, 6, 8, 10], random_state=0)  #instead of passing out i need to hardcode it.\n",
    "\n",
    "#     X_resampled, y_resampled = smote_nc.fit_resample(train,y)\n",
    "    \n",
    "#     print(sorted(Counter(y_resampled).items()))\n",
    "    \n",
    "#     return X_resampled,y_resampled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glt8kTxZLq1-"
   },
   "outputs": [],
   "source": [
    "#####crete new features###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTTM9ah-Lq2F"
   },
   "outputs": [],
   "source": [
    "def createIntercations(train,test,cat_features):\n",
    "    import itertools\n",
    "    print(f\"Creating features on {cat_features}, with combination 2 for training data /n\")\n",
    "    interactionstrain = pd.DataFrame(index=train.index)\n",
    "    \n",
    "    for col1 ,col2 in  itertools.combinations(cat_features,2):   \n",
    "        newcolname = col1 + \"_\" + col2 \n",
    "        new_values = train[col1].map(str) + \"_\" + train[col2].map(str)\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "    \n",
    "    train_df = train.join(interactionstrain)\n",
    "    \n",
    "    print(f\"Creating features on {cat_features}, with combination 2 for testing data\")\n",
    "    interactionstest = pd.DataFrame(index=train.index)\n",
    "    \n",
    "    for col1 ,col2 in  itertools.combinations(cat_features,2):   \n",
    "        newcolname = col1 + \"_\" + col2 \n",
    "        new_values = test[col1].map(str) + \"_\" + test[col2].map(str)\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "    test_df = test.join(interactionstest)\n",
    "    \n",
    "    print(train_df.shape,test_df.shape)\n",
    "    \n",
    "    \n",
    "    return train_df,test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d6CI5m2cLq2K"
   },
   "outputs": [],
   "source": [
    "def createmeanfestures(train,test ,features):\n",
    "  \n",
    "  cols = features\n",
    "  interactionstrain = pd.DataFrame(index=train.index)\n",
    "  interactionstest = pd.DataFrame(index=test.index)\n",
    "\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_mean\"   \n",
    "        new_values = train[col].mean()\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_mean\"   \n",
    "        new_values = test[col].mean()\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "\n",
    "  train_df = train.join(interactionstrain)\n",
    "  test_df = test.join(interactionstest)\n",
    "\n",
    "  print(train_df.shape,test_df.shape)\n",
    "\n",
    "  return train_df,test_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createlogfestures(train,test,features):\n",
    "  \n",
    "  cols = features\n",
    "  interactionstrain = pd.DataFrame(index=train.index)\n",
    "  interactionstest = pd.DataFrame(index=test.index)\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_log\"   \n",
    "        new_values = np.log1p(train[col])\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_log\"   \n",
    "        new_values = np.log1p(test[col])\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "\n",
    "  train_df = train.join(interactionstrain)\n",
    "  test_df = test.join(interactionstest)\n",
    "\n",
    "  print(train_df.shape,test_df.shape)\n",
    "\n",
    "\n",
    "  return train_df ,test_df\n",
    "\n",
    "def createsqrtfeatures(train,test,features):\n",
    "    \n",
    "\n",
    "  cols = features\n",
    "  interactionstrain = pd.DataFrame(index=train.index)\n",
    "  interactionstest = pd.DataFrame(index=test.index)\n",
    "  \n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_sqrt\"   \n",
    "        new_values = np.sqrt(train[col])\n",
    "        interactionstrain[newcolname] = new_values\n",
    "\n",
    "  for col  in cols:   \n",
    "        newcolname = col + \"_sqrt\"   \n",
    "        new_values = np.sqrt(test[col])\n",
    "        interactionstest[newcolname] = new_values\n",
    "\n",
    "\n",
    "  train_df = train.join(interactionstrain)\n",
    "  test_df = test.join(interactionstest)\n",
    "\n",
    "  print(train_df.shape,test_df.shape)\n",
    "\n",
    "\n",
    "  return train_df ,test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4qIejjhLq2O"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import classification_report\n",
    "    \n",
    "# Function for comparing different approaches\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    #model = RandomForestClassifier(random_state=0)\n",
    "    #model.fit(X_train, y_train)\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "    clf1 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=None, max_features=25, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=8, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "    clf2 = BaggingClassifier(base_estimator=clf1, n_estimators=10, \n",
    "                             bootstrap=True,\n",
    "                             bootstrap_features=False, oob_score=True, warm_start=False,\n",
    "                             n_jobs=-1, random_state=786, verbose=0)\n",
    "    clf2.fit(X_train,y_train)\n",
    "\n",
    "    preds = clf2.predict(X_valid)\n",
    "    target_names = ['class 0', 'class 1', 'class 2','class 3', 'class 4', 'class 5', 'class 6']\n",
    "    print(classification_report(y_valid, preds, target_names=target_names,labels= [0,1,2,3,4,5,6]))\n",
    "    return clf2.score(X_valid,y_valid) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jG0IPOVrLq2T"
   },
   "outputs": [],
   "source": [
    "# X_train, X_valid, y_train, y_valid\n",
    "#clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Inspection_Results\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "train = data.drop(['Inspection_Results'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "3aCJ8JExLq2X",
    "outputId": "01e10416-b376-4745-ff76-9a5d40cf0005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 18) (49148, 18)\n"
     ]
    }
   ],
   "source": [
    "trainS, testS = special(train ,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Os_fd0AYLq2e",
    "outputId": "76b63eee-8257-4db7-96ca-2482c23dfb72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 17) (49148, 17)\n"
     ]
    }
   ],
   "source": [
    "trainSd, testSd = dropcol(trainS,testS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ro7b9oEqLq2k",
    "outputId": "d2dd3845-23e1-4a62-c9af-ca38ac559e5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 17) (49148, 17)\n"
     ]
    }
   ],
   "source": [
    "trainSdIm, testSdIm = imputer(trainSd, testSd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 612
    },
    "colab_type": "code",
    "id": "dxkAfjvCLq2q",
    "outputId": "0f2cd5fe-7e65-4330-de46-678b6cc69c44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                   Total    %\n",
       " logID                  0  0.0\n",
       " LocationID             0  0.0\n",
       " FacilityID             0  0.0\n",
       " FacilityName           0  0.0\n",
       " Type                   0  0.0\n",
       " Street                 0  0.0\n",
       " City                   0  0.0\n",
       " State                  0  0.0\n",
       " Reason                 0  0.0\n",
       " year                   0  0.0\n",
       " SectionViolations      0  0.0\n",
       " RiskLevel              0  0.0\n",
       " Geo_Loc                0  0.0\n",
       " hour                   0  0.0\n",
       " day                    0  0.0\n",
       " month                  0  0.0\n",
       " LicenseNo              0  0.0,                    Total    %\n",
       " logID                  0  0.0\n",
       " LocationID             0  0.0\n",
       " FacilityID             0  0.0\n",
       " FacilityName           0  0.0\n",
       " Type                   0  0.0\n",
       " Street                 0  0.0\n",
       " City                   0  0.0\n",
       " State                  0  0.0\n",
       " Reason                 0  0.0\n",
       " year                   0  0.0\n",
       " SectionViolations      0  0.0\n",
       " RiskLevel              0  0.0\n",
       " Geo_Loc                0  0.0\n",
       " hour                   0  0.0\n",
       " day                    0  0.0\n",
       " month                  0  0.0\n",
       " LicenseNo              0  0.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingcheck(trainSdIm),missingcheck(testSdIm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "2aw1FGA0Lq2u",
    "outputId": "c38efd08-d954-41a7-a794-b64e8b14eb12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features on ['SectionViolations', 'RiskLevel', 'Reason'], with combination 2 for training data /n\n",
      "Creating features on ['SectionViolations', 'RiskLevel', 'Reason'], with combination 2 for testing data\n",
      "(147443, 20) (49148, 20)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn1,testSdImIn1 = createIntercations(trainSdIm, testSdIm,cat_features = ['SectionViolations','RiskLevel','Reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "3kYy-ZzwLq2y",
    "outputId": "372d0c76-8130-4dca-be62-f6c58ee192de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating features on ['State', 'City', 'Street', 'LocationID', 'Geo_Loc'], with combination 2 for training data /n\n",
      "Creating features on ['State', 'City', 'Street', 'LocationID', 'Geo_Loc'], with combination 2 for testing data\n",
      "(147443, 30) (49148, 30)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12,testSdImIn12 = createIntercations(trainSdImIn1,testSdImIn1,cat_features = ['State','City','Street','LocationID','Geo_Loc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jaK3rthjLq22"
   },
   "outputs": [],
   "source": [
    "# X_trainSDrIMintertgenc,X_validSDrIMinter2tgenc = targetencoding(X_trainSDrIMinter2,X_validSDrIMinter2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "yWqJ47VYLq29",
    "outputId": "55b003e3-1719-4c87-9bf8-431d9b54cd12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Reason', 'SectionViolations_RiskLevel', 'SectionViolations_Reason', 'RiskLevel_Reason', 'State_Street', 'State_LocationID', 'State_Geo_Loc', 'City_Street', 'City_LocationID', 'City_Geo_Loc', 'Street_LocationID', 'Street_Geo_Loc', 'LocationID_Geo_Loc']\n",
      "(147443, 30) (49148, 30)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12lb,testSdImIn12lb = robustlabelencoder(trainSdImIn12,testSdImIn12 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zqc8oJbZLq3C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-VY5Kj5Lq3G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "u3mh8M9DLq3J",
    "outputId": "7ec7345a-4dec-4561-c745-5e05383d944c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['City', 'State', 'RiskLevel', 'State_City']\n",
      "(147443, 38) (49148, 38)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12lbohe,testSdImIn12lbohe = onhotencoder(trainSdImIn12lb,testSdImIn12lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "8SeFb6P7wSDj",
    "outputId": "5246a922-5689-41e3-f144-15112a193360"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['LicenseNo', 'FacilityID', 'FacilityName', 'Type', 'Street',\n",
       "       'LocationID', 'Reason', 'SectionViolations', 'Geo_Loc', 'hour', 'day',\n",
       "       'month', 'year', 'logID', 'SectionViolations_RiskLevel',\n",
       "       'SectionViolations_Reason', 'RiskLevel_Reason', 'State_Street',\n",
       "       'State_LocationID', 'State_Geo_Loc', 'City_Street', 'City_LocationID',\n",
       "       'City_Geo_Loc', 'Street_LocationID', 'Street_Geo_Loc',\n",
       "       'LocationID_Geo_Loc', 'City_id-11235901', 'City_id-11275913',\n",
       "       'State_id_1890134', 'State_id_1890135', 'RiskLevel_High',\n",
       "       'RiskLevel_Low', 'RiskLevel_Medium', 'RiskLevel_Uncertain',\n",
       "       'State_City_id_1890134_id-11235901',\n",
       "       'State_City_id_1890134_id-11275913',\n",
       "       'State_City_id_1890135_id-11235901',\n",
       "       'State_City_id_1890135_id-11275913'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = trainSdImIn12lbohe.columns\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7VcTkYBfuoxw",
    "outputId": "ab52ef67-905f-4a21-a033-af890abee230"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZjbAtYuUuo_P",
    "outputId": "0dc1abab-9ba4-4962-ab73-7b874577c1a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 76) (49148, 76)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12lboheLog,testSdImIn12lboheLog = createlogfestures(trainSdImIn12lbohe,testSdImIn12lbohe,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PnoVbxPJ0S0H",
    "outputId": "ed8f2533-2ddc-4587-f9b1-dd7a3864d5a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147443, 114) (49148, 114)\n"
     ]
    }
   ],
   "source": [
    "trainSdImIn12lboheLogSQ,testSdImIn12lboheLogSQ = createsqrtfeatures(trainSdImIn12lboheLog,testSdImIn12lboheLog,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGt_o723Lq4e"
   },
   "outputs": [],
   "source": [
    "def featureselection(train,y_train):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import ExtraTreesClassifier\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    from lightgbm import LGBMClassifier\n",
    "    from sklearn.feature_selection import chi2\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.feature_selection import RFE\n",
    "    from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "    feature_name = train.columns.tolist()\n",
    "    def cor_selector(X, y):\n",
    "        cor_list = []\n",
    "        feature_name = X.columns.tolist()\n",
    "\n",
    "        # calculate the correlation with y for each feature\n",
    "        for i in X.columns.tolist():\n",
    "            cor = np.corrcoef(X[i], y)[0, 1]\n",
    "            cor_list.append(cor)\n",
    "        # replace NaN with 0\n",
    "        cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
    "        # feature name\n",
    "        cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-100:]].columns.tolist()\n",
    "        # feature selection? 0 for not select, 1 for select\n",
    "        cor_support = [True if i in cor_feature else False for i in feature_name]\n",
    "        return cor_support, cor_feature\n",
    "\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=7 )\n",
    "    model_rf = SelectFromModel(rf ,threshold ='1.25*median')\n",
    "    X_new_rf = model_rf.fit(train,y_train)\n",
    "    embeded_rf_support = model_rf.get_support()\n",
    "    embeded_rf_feature = train.loc[:,embeded_rf_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    model_lgbc = SelectFromModel(lgbc,threshold = '1.25*median')\n",
    "    X_new_lgbc = model_lgbc.fit(train,y_train)\n",
    "    lgbc_rf_support = model_lgbc.get_support()\n",
    "    lgbc_rf_feature = train.loc[:,lgbc_rf_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    cor_support, cor_feature = cor_selector(train, y_train)\n",
    "    \n",
    "    \n",
    "    X_norm = MinMaxScaler().fit_transform(train)\n",
    "    \n",
    "    chi_selector = SelectKBest(chi2, k=80)\n",
    "    chi_selector.fit(X_norm, y_train)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = train.loc[:,chi_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=100, step=10, verbose=5)\n",
    "    rfe_selector.fit(X_norm, y_train)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = train.loc[:,rfe_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\",solver = 'liblinear'), '1.25*median')\n",
    "    embeded_lr_selector.fit(X_norm, y_train)\n",
    "\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    embeded_lr_feature = train.loc[:,embeded_lr_support].columns.tolist()\n",
    "    \n",
    "    \n",
    "    pd.set_option('display.max_rows', None)\n",
    "    \n",
    "    feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n",
    "                                        'Random Forest':embeded_rf_support, 'LightGBM':lgbc_rf_support})\n",
    "    \n",
    "    feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
    "    feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
    "    feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
    "    feature_selection_df.head(100)\n",
    "    print(feature_selection_df.columns)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/.local/lib/python3.6/site-packages/numpy/lib/function_base.py:2530: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[:, None]\n",
      "/home/ryan/.local/lib/python3.6/site-packages/numpy/lib/function_base.py:2531: RuntimeWarning: invalid value encountered in true_divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 114 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 104 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "featureselection(trainSdImIn12lboheLogSQ,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "clf1 = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
    "                       max_depth=None, max_features=80, max_leaf_nodes=None,\n",
    "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                       min_samples_leaf=8, min_samples_split=2,\n",
    "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
    "                       random_state=None, splitter='best')\n",
    "\n",
    "clf2 = BaggingClassifier(base_estimator=clf1, n_estimators=100, \n",
    "                             bootstrap=True,\n",
    "                             bootstrap_features=False, oob_score=True, warm_start=False,\n",
    "                             n_jobs=-1, random_state=786, verbose=0)\n",
    "\n",
    "# clf3=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#               colsample_bynode=1, colsample_bytree=0.8, gamma=0.5,\n",
    "#               learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
    "#               min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
    "#               nthread=None, objective='multi:softprob', random_state=0,\n",
    "#               reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "#               silent=None, subsample=0.6, verbosity=1)\n",
    "\n",
    "\n",
    "clf4 = lgbc=LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2,\n",
    "            reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:633: UserWarning: Some inputs do not have OOB scores. This probably means too few estimators were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_bagging.py:638: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions.sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "clf2.fit(trainSdImIn12lboheLogSQ,y)\n",
    "\n",
    "pred1 = clf2.predict_proba(testSdImIn12lboheLogSQ)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3.fit(trainSdImIn12lboheLogSQ,y)\n",
    "\n",
    "pred2 = clf3.predict_proba(testSdImIn12lboheLogSQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4.fit(trainSdImIn12lboheLogSQ,y)\n",
    "\n",
    "pred3 = clf4.predict_proba(testSdImIn12lboheLogSQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub1 = pd.DataFrame(data=np.mean([pred1, pred2,pred3], axis=0)\n",
    "                         , columns=submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub2 = pd.DataFrame(data=np.mean([pred1,pred3], axis=0)\n",
    "                         , columns=submission.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub1.to_excel(excel_writer = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/sub1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xt-Op3_XDPCI"
   },
   "outputs": [],
   "source": [
    "final_sub2.to_excel(excel_writer = \"/home/ryan/stark/MachineHack/Food_QUalityA_ParticipantsData/sub2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sub3 = pd.DataFrame(da, axis=0)\n",
    "                         , columns=submission.columns)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "awaybasic-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
